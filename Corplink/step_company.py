# coding: utf-8
import re
from typing import List, Dict, Set

import pandas as pd
from sqlalchemy import create_engine, text
from tqdm import tqdm
import numpy as np
from rapidfuzz import fuzz, process

from .env_bootstrap import cute_box
from .constants import BASE_DIR, MAX_COMP_COLS
from . import state
from .model_utils import nlp, model_emb, calc_Bad_Score
from .text_utils import is_valid_token

def dedup_company_cols(df: pd.DataFrame) -> pd.DataFrame:
    comp_cols = [c for c in df.columns if c.startswith("company_")]
    for ridx in df.index:
        seen: Set[str] = set()
        for col in comp_cols:
            val = str(df.at[ridx, col]).strip()
            if val in seen:
                df.at[ridx, col] = ""
            else:
                seen.add(val)
    return df

def extract_companies(text: str,
                      company_db: List[str],
                      ner_model,
                      fuzzy_threshold: int = 95) -> List[str]:
    comps: Set[str] = set()

    text_clean = re.sub(r"\s*\d{1,2}/\d{1,2}/\d{2,4}.*$", "", text).strip()
    text_clean = re.sub(r"[Â®â„¢Â©]", "", text_clean)
    text_clean = re.sub(r"\(\s*[A-Z]{1,3}\s*\)", "", text_clean)
    text_clean = re.sub(r"\b\S+@\S+\b", "", text_clean)

    doc = ner_model(text_clean)
    for ent in doc.ents:
        ent_text = ent.text.strip()

        if "  " in ent_text or re.search(r"[\d/%+]|[^\x00-\x7F]", ent_text):
            continue
        valid_ent = True
        for w in ent_text.split():
            if (not w[0].isalpha()
                or w in {"The","And","For","With","From","That","This"}
                or not is_valid_token(w)):
                valid_ent = False
                break
        if valid_ent:
            comps.add(ent_text)

    for m in re.findall(r"\b([A-Z]{2,})ers\b", text_clean):
        comps.add(m)

    STOPWORDS = {"The","And","For","With","From","That","This","Have","Will",
                "Are","You","Not","But","All","Any","One","Our","Their"}

    tokens = re.findall(r"\b\S+\b", text_clean)
    for pos, token in enumerate(tokens):
        if (pos == 0 or token in STOPWORDS
            or any(ch in token for ch in "/%+") or "  " in token
            or len(token) < 5 or not token[0].isupper() or token.isupper()
            or re.search(r"\d|[^\x00-\x7F]", token)
            or not is_valid_token(token)):
            continue

        if any(token.lower() == db.lower() for db in company_db):
            comps.add(token)

    return list(comps)

def step2(mysql_url: str):
    cute_box(
        "Step-2ï¼šå…¬å¸è¯†åˆ«ï¼‹BAN è¿‡æ»¤ ä¸­â€¦",
        "Step-2ï¼šä¼æ¥­åèªè­˜ï¼‹BAN ãƒ•ã‚£ãƒ«ã‚¿ä¸­â€¦",
        "ğŸ·ï¸"
    )
    engine_tmp = create_engine(mysql_url)
    df_canon = pd.read_sql("SELECT id, canonical_name FROM company_canonical", engine_tmp)
    df_canon.to_csv(BASE_DIR / "canonical_list.csv", index=False, encoding="utf-8-sig")
    cute_box(
        f"å·²å†™å‡º canonical_list.csvï¼Œå…± {len(df_canon)} è¡Œ",
        f"canonical_list.csv ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼š{len(df_canon)} è¡Œ",
        "ğŸ—‚ï¸"
    )

    engine = create_engine(mysql_url)
    with engine.begin() as conn:
        ban_set = {r[0] for r in conn.execute(text("SELECT alias FROM ban_list"))}
        rows = conn.execute(text("""
            SELECT a.alias, c.canonical_name FROM company_alias a
            JOIN company_canonical c ON a.canonical_id = c.id
        """))
        alias_map = {alias: canon for alias, canon in rows}
        canon_set = {r[0] for r in conn.execute(text("SELECT canonical_name FROM company_canonical"))}
        canon_names = list(canon_set)
        canon_vecs  = model_emb.encode(canon_names, batch_size=64, normalize_embeddings=True)
        rows2 = conn.execute(text(
            "SELECT id, canonical_name FROM company_canonical"
        ))
        canon_name2id = {name: cid for cid, name in rows2}
    
    cute_box(
    f"ban_list={len(ban_set)}ï¼Œalias_map={len(alias_map)}ï¼Œcanon_set={len(canon_set)}",
    f"ban_listï¼š{len(ban_set)}ä»¶ï¼alias_mapï¼š{len(alias_map)}ä»¶ï¼canon_setï¼š{len(canon_set)}ä»¶",
    "ğŸ”"
    )

    df = pd.DataFrame(state.SENTENCE_RECORDS)
    df_hit = df[df["Hit_Count"].astype(int) >= 1].reset_index(drop=True)
    if df_hit.empty:
        cute_box(
        "Step-1 æ²¡æå–åˆ°ä»»ä½•å¥å­ï¼Œè¯·å…ˆè·‘ Step-1ï¼",
        "Step-1 ã§æ–‡ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã¾ãš Step-1 ã‚’å®Ÿè¡Œã—ã¦ã­",
        "ğŸš«"
        )
        return

    company_db = list(canon_set) + list(alias_map.keys())
    comp_cols: List[List[str]] = []
    for sent in tqdm(df_hit["Sentence"].tolist(), desc="å…¬å¸è¯†åˆ«"):
        names_raw = extract_companies(sent, company_db, nlp)
        uniq: List[str] = []
        for alias in names_raw:
            if alias in uniq:     
                continue
            uniq.append(alias)
        comp_cols.append(uniq[:MAX_COMP_COLS])

    for i in range(MAX_COMP_COLS):
        df_hit[f"company_{i+1}"] = [lst[i] if i < len(lst) else "" for lst in comp_cols]
        
    ban_lower     = {b.lower() for b in ban_set}
    canon_lower   = {c.lower() for c in canon_set}
    alias_lower   = {a.lower(): canon for a, canon in alias_map.items()}
    canon_lower2orig = {c.lower(): c for c in canon_set}

    def _norm_key(s: str) -> str:
        return re.sub(r"[^A-Za-z0-9]", "", s).lower()

    comp_cols = [f"company_{i+1}" for i in range(MAX_COMP_COLS)]

    for ridx in df_hit.index:
        orig_names = [df_hit.at[ridx, c].strip() for c in comp_cols if df_hit.at[ridx, c].strip()]
        new_names  = []
        for nm in orig_names:
            nm_l = nm.lower()
            if nm_l in ban_lower:
                continue
            if nm_l in canon_lower:
                new_names.append(canon_lower2orig[nm_l])
                continue
            if nm_l in alias_lower:
                new_names.append(alias_lower[nm_l])
                continue
            new_names.append(nm)

        cleaned = []
        seen_keys = set()
        for nm in sorted(new_names, key=len, reverse=True):
            key = _norm_key(nm)
            if any(key in k or k in key for k in seen_keys):
                continue
            cleaned.append(nm)
            seen_keys.add(key)
        for i, col in enumerate(comp_cols):
            df_hit.at[ridx, col] = cleaned[i] if i < len(cleaned) else ""

    meta_cols = ["Tier_1", "Tier_2", "Filename", "Date",
                 "Title", "Publisher", "Sentence",
                 "Hit_Count", "Matched_Keywords"]

    df_final = (df_hit[meta_cols +
                [c for c in df_hit.columns if c.startswith("company_")]]
                .fillna(""))
    df_final = dedup_company_cols(df_final)

    df_final.to_csv(BASE_DIR / "result.csv",
                    index=False, encoding="utf-8-sig")
    cute_box(
        f"å·²ç”Ÿæˆ result.csvï¼Œå…± {len(df_final)} æ¡è®°å½•",
        f"result.csv ã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼šå…¨{len(df_final)}ä»¶",
        "ğŸ“‘"
    )

    canon_name2id = {row.canonical_name: row.id for row in df_canon.itertuples()}

    todo_rows: List[Dict] = []
    ban_hits = alias_hits = canon_hits = 0
    rows_skipped_not_enough_companies = 0

    comp_cols = [c for c in df_final.columns if c.startswith("company_")]

    for _, row in df_final.iterrows():
        names = [row[c].strip() for c in comp_cols if row[c].strip()]

        unknowns: List[str] = []
        for alias in names:
            alias_l = alias.lower()
            if alias_l in ban_lower:
                ban_hits += 1
                continue
            if alias_l in alias_lower:
                alias_hits += 1
                continue
            if alias_l in canon_lower:
                canon_hits += 1
                continue
            unknowns.append(alias)

        if len(names) < 2:
            rows_skipped_not_enough_companies += 1
            continue

        if len(canon_vecs) > 0 and unknowns:
            unknown_vecs = model_emb.encode(unknowns, normalize_embeddings=True)
        else:
            unknown_vecs = []

        for i, alias in enumerate(unknowns):
            advice = ""
            adviced_id = ""
            match_info = ""
            
            fuzzy_res = process.extractOne(alias, canon_names, scorer=fuzz.token_sort_ratio)
            
            if fuzzy_res:
                candidate, score, _ = fuzzy_res
                if score >= 90:
                    advice = candidate
                    adviced_id = canon_name2id.get(advice, "")
                    match_info = f"Fuzzy({score:.0f})"
            
            if not advice and len(canon_vecs) > 0:
                curr_vec = unknown_vecs[i]
                sims = np.dot(canon_vecs, curr_vec)
                best_idx = int(np.argmax(sims))
                vector_score = float(sims[best_idx])
                
                if vector_score >= 0.82:
                    advice = canon_names[best_idx]
                    adviced_id = canon_name2id.get(advice, "")
                    match_info = f"AI({vector_score:.2f})"

            todo_rows.append({
                "Sentence": row["Sentence"],
                "Alias":    alias,
                "Bad_Score": calc_Bad_Score(alias),
                "Advice":   advice,
                "Adviced_ID": adviced_id,
                "Canonical_Name": "",
                "Std_Result": ""
            })

    todo_cols = [
        "Sentence", "Alias", "Bad_Score",
        "Advice", "Adviced_ID",
        "Canonical_Name", "Std_Result"
    ]

    if not todo_rows:
        todo_df = pd.DataFrame(columns=todo_cols)
        todo_df.to_csv(BASE_DIR / "result_mapping_todo.csv",
                       index=False, encoding="utf-8-sig")

        cute_box(
            "æœ¬æ‰¹æ²¡æœ‰äº§ç”Ÿæ–°çš„åˆ«åéœ€è¦æ˜ å°„ï¼›å·²è¢«è§„åˆ™è¯†åˆ«/è¿‡æ»¤ï¼Œæˆ–å› â€œåŒè¡Œå…¬å¸ä¸è¶³ï¼ˆ<2ï¼‰â€è§„åˆ™è€Œè·³è¿‡ã€‚\n"
            f"ï¼ˆban å‘½ä¸­ï¼š{ban_hits}ï¼Œå·²æœ‰ aliasï¼š{alias_hits}ï¼Œå·²æœ‰ canonicalï¼š{canon_hits}ï¼ŒåŒè¡Œå…¬å¸ä¸è¶³è·³è¿‡ï¼š{rows_skipped_not_enough_companies}ï¼‰",
            "ä»Šå›ã®ãƒãƒƒãƒã§ã¯æ–°ã—ã„åˆ¥åã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«ä¸€è‡´ï¼é™¤å¤–ã€ã¾ãŸã¯ã€ŒåŒä¸€è¡Œã®ä¼æ¥­æ•°ãŒ2æœªæº€ã€è¦å‰‡ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚\n"
            f"ban ä¸€è‡´ï¼š{ban_hits}ï¼æ—¢å­˜ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼š{alias_hits}ï¼æ—¢å­˜ã‚«ãƒãƒ‹ã‚«ãƒ«ï¼š{canon_hits}ï¼åŒä¸€è¡Œã®ä¼æ¥­æ•°ä¸è¶³ã‚¹ã‚­ãƒƒãƒ—ï¼š{rows_skipped_not_enough_companies}",
            "â„¹ï¸"
        )
    else:
        todo_df = pd.DataFrame(todo_rows)
        todo_df["__alias_l"] = todo_df["Alias"].str.lower()
        todo_df = todo_df.drop_duplicates("__alias_l").drop(columns="__alias_l")

        todo_df["__grp"] = todo_df["Bad_Score"].apply(lambda x: 0 if x >= 50 else (1 if x >= 10 else 2))
        todo_df = (todo_df
                   .sort_values(["__grp", "Sentence"], ascending=[True, True])
                   .drop(columns="__grp"))

        for col in todo_cols:
            if col not in todo_df.columns:
                todo_df[col] = ""
        todo_df = todo_df[todo_cols]

        todo_df["Bad_Score"] = todo_df["Bad_Score"].astype(int).astype(str)
        todo_df['Sentence'] = todo_df['Sentence'].apply(
            lambda s: "'" + s if isinstance(s, str) and s.startswith('=') else s
        )
        todo_df.to_csv(BASE_DIR / "result_mapping_todo.csv",
                       index=False, encoding="utf-8-sig")

        cute_box(
            f"å·²ç”Ÿæˆ result_mapping_todo.csvï¼Œå…± {len(todo_df)} æ¡å¾…å¤„ç†åˆ«åã€‚\n"
            f"ï¼ˆban å‘½ä¸­ï¼š{ban_hits}ï¼Œå·²æœ‰ aliasï¼š{alias_hits}ï¼Œå·²æœ‰ canonicalï¼š{canon_hits}ï¼ŒåŒè¡Œå…¬å¸ä¸è¶³è·³è¿‡ï¼š{rows_skipped_not_enough_companies}ï¼‰",
            f"result_mapping_todo.csv ã‚’ä½œæˆï¼š{len(todo_df)} ä»¶ã®å€™è£œã€‚\n"
            f"ï¼ˆban ä¸€è‡´ï¼š{ban_hits}ï¼æ—¢å­˜ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼š{alias_hits}ï¼æ—¢å­˜ã‚«ãƒãƒ‹ã‚«ãƒ«ï¼š{canon_hits}ï¼åŒä¸€è¡Œã®ä¼æ¥­æ•°ä¸è¶³ã‚¹ã‚­ãƒƒãƒ—ï¼š{rows_skipped_not_enough_companies}ï¼‰",
            "ğŸ“"
        )
        
    cute_box(
    "Step-2 å®Œæˆï¼è¯·ç¼–è¾‘ result_mapping_todo.csv ç„¶åè¿è¡Œ Step-3",
    "Step-2 å®Œäº†ï¼result_mapping_todo.csv ã‚’ç·¨é›†ã—ã¦ã‹ã‚‰ Step-3 ã‚’å®Ÿè¡Œã—ã¦ã­",
    "âœ…"
    )
    cute_box(
        "result_mapping_todo.csv å¿«é€Ÿå¡«å†™æŒ‡å—ï¼š\n"
        "1) ç©ºç™½â†’è·³è¿‡\n"
        "2) 0â†’åŠ  ban_list\n"
        "3) nâ†’è§†ä¸º canonical_id\n"
        "4) å…¶ä»–â†’æ–°/å·²æœ‰æ ‡å‡†å",
        "result_mapping_todo.csv ç°¡æ˜“å…¥åŠ›ã‚¬ã‚¤ãƒ‰ï¼š\n"
        "1) ãƒ–ãƒ©ãƒ³ã‚¯â†’ã‚¹ã‚­ãƒƒãƒ—\n"
        "2) 0â†’ban_listç™»éŒ²\n"
        "3) nâ†’canonical_id ã¨è¦‹ãªã™\n"
        "4) ãã®ä»–â†’æ–°è¦/æ—¢å­˜æ¨™æº–å",
        "ğŸ“‹"
    )